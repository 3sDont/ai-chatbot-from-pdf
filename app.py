# app.py

import streamlit as st
import os
import sys
import base64 # ThÆ° viá»‡n cáº§n thiáº¿t Ä‘á»ƒ hiá»ƒn thá»‹ PDF

# --- ThÃªm thÆ° má»¥c gá»‘c vÃ o sys.path ---
project_root = os.path.dirname(os.path.abspath(__file__))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# --- Import tá»« kiáº¿n trÃºc má»›i ---
from src.components.data_loader import DataLoader
from src.components.chunker import Chunker
from src.components.embedder import Embedder
from src.components.vector_store import VectorStore
from src.pipelines.llm_models import GroqLLM
from src.pipelines.rag_pipeline import RAGPipeline

# --- Cáº¥u hÃ¬nh trang ---
st.set_page_config(page_title="ğŸ“š AI Chatbot Pro", layout="wide", initial_sidebar_state="expanded")

# --- HÃ€M TIá»†N ÃCH Äá»‚ HIá»‚N THá»Š PDF ---
def display_pdf(file):
    """
    Hiá»ƒn thá»‹ má»™t file PDF trong Streamlit báº±ng cÃ¡ch nhÃºng nÃ³ vÃ o iframe.
    """
    # Äá»c file
    file.seek(0)
    base64_pdf = base64.b64encode(file.read()).decode('utf-8')
    # NhÃºng vÃ o iframe
    pdf_display = f'<iframe src="data:application/pdf;base64,{base64_pdf}" width="100%" height="800" type="application/pdf"></iframe>'
    # Hiá»ƒn thá»‹
    st.markdown(pdf_display, unsafe_allow_html=True)

# --- KHá»I Táº O CÃC Äá»I TÆ¯á»¢NG (DÃ™NG CACHE) ---
@st.cache_resource
def initialize_models():
    try:
        embedder = Embedder()
        llm = GroqLLM()
        return embedder, llm
    except ValueError as e:
        st.error(f"Lá»—i khá»Ÿi táº¡o: {e}. Vui lÃ²ng kiá»ƒm tra API key trong Streamlit Secrets.")
        return None, None

def initialize_rag_pipeline(embedder, llm):
    return RAGPipeline(Chunker(), embedder, VectorStore(), llm)

# Táº£i model vÃ  kiá»ƒm tra lá»—i
embedder_model, llm_model = initialize_models()
if not (embedder_model and llm_model):
    st.stop()

# Khá»Ÿi táº¡o pipeline vÃ  cÃ¡c biáº¿n session state
if 'rag_pipeline' not in st.session_state:
    st.session_state.rag_pipeline = initialize_rag_pipeline(embedder_model, llm_model)
if 'messages' not in st.session_state:
    st.session_state.messages = []
if 'document_processed' not in st.session_state:
    st.session_state.document_processed = False
if 'uploaded_file' not in st.session_state:
    st.session_state.uploaded_file = None

# --- GIAO DIá»†N SIDEBAR Äá»‚ UPLOAD ---
with st.sidebar:
    st.header("âš™ï¸ Báº£ng Äiá»u Khiá»ƒn")
    uploaded_file = st.file_uploader("Táº£i lÃªn tÃ i liá»‡u PDF cá»§a báº¡n", type=["pdf"])

    if uploaded_file:
        # Xá»­ lÃ½ chá»‰ khi cÃ³ file má»›i Ä‘Æ°á»£c táº£i lÃªn
        if st.session_state.uploaded_file is None or st.session_state.uploaded_file.name != uploaded_file.name:
            st.session_state.uploaded_file = uploaded_file
            st.session_state.document_processed = False
            st.session_state.messages = [] # Reset chat khi cÃ³ file má»›i

            with st.status("âš™ï¸ Äang xá»­ lÃ½ tÃ i liá»‡u...", expanded=True) as status:
                status.write("Äang Ä‘á»c file...")
                loader = DataLoader()
                # Chá»‰ dÃ¹ng luá»“ng xá»­ lÃ½ PDF nhanh cho giao diá»‡n tÆ°Æ¡ng tÃ¡c nÃ y
                content = loader.load_from_upload(uploaded_file)
                
                if content:
                    status.write("Äang phÃ¢n tÃ­ch vÃ  ghi nhá»› ná»™i dung...")
                    st.session_state.rag_pipeline = initialize_rag_pipeline(embedder_model, llm_model)
                    st.session_state.rag_pipeline.setup_with_text(content)
                    st.session_state.document_processed = True
                    status.update(label="âœ… Xá»­ lÃ½ hoÃ n táº¥t!", state="complete", expanded=False)
                else:
                    status.update(label="Lá»—i Ä‘á»c file", state="error")
    
    st.markdown("---")
    st.markdown(
        "**LÆ°u Ã½:** Chá»©c nÄƒng xem trÆ°á»›c PDF vÃ  há»i Ä‘Ã¡p nhanh hoáº¡t Ä‘á»™ng tá»‘t nháº¥t vá»›i cÃ¡c file PDF cÃ³ text rÃµ rÃ ng."
    )

# --- Bá» Cá»¤C GIAO DIá»†N CHÃNH (2 Cá»˜T) ---
if st.session_state.uploaded_file is None:
    st.info("ChÃ o má»«ng báº¡n! Vui lÃ²ng táº£i lÃªn má»™t tÃ i liá»‡u PDF á»Ÿ thanh bÃªn Ä‘á»ƒ báº¯t Ä‘áº§u.")
else:
    # Táº¡o hai cá»™t: má»™t cho hiá»ƒn thá»‹ PDF, má»™t cho chatbot
    col1, col2 = st.columns([1, 1]) # Tá»‰ lá»‡ 1:1

    # Cá»™t 1: Hiá»ƒn thá»‹ PDF
    with col1:
        st.subheader(f"ğŸ“„ Ná»™i dung tÃ i liá»‡u: {st.session_state.uploaded_file.name}")
        display_pdf(st.session_state.uploaded_file)

    # Cá»™t 2: Giao diá»‡n Chat
    with col2:
        st.subheader("ğŸ¤– Chat vá»›i AI")
        
        # VÃ¹ng chá»©a tin nháº¯n
        chat_container = st.container()
        with chat_container:
            for message in st.session_state.messages:
                with st.chat_message(message["role"]):
                    st.markdown(message["content"])

        # Ã” nháº­p liá»‡u chat
        if prompt := st.chat_input("Äáº·t cÃ¢u há»i vá» tÃ i liá»‡u..."):
            st.session_state.messages.append({"role": "user", "content": prompt})
            with chat_container:
                with st.chat_message("user"):
                    st.markdown(prompt)

            if not st.session_state.document_processed:
                st.warning("TÃ i liá»‡u chÆ°a Ä‘Æ°á»£c xá»­ lÃ½ xong. Vui lÃ²ng chá».")
            else:
                with chat_container:
                    with st.chat_message("assistant"):
                        with st.spinner("AI Ä‘ang suy nghÄ©..."):
                            response = st.session_state.rag_pipeline.query(prompt)
                            st.markdown(response)
                st.session_state.messages.append({"role": "assistant", "content": response})
