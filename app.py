# app.py

import streamlit as st
import os

# --- CÃC Lá»†NH IMPORT ÄÃƒ ÄÆ¯á»¢C Sá»¬A ---
# KhÃ´ng cÃ²n 'src.' á»Ÿ Ä‘áº§u ná»¯a vÃ¬ cÃ¡c file Ä‘Ã£ ngang hÃ ng
from pdf_reader import PDFReader
from text_splitter import TextSplitter
from embedder import Embedder
from vector_store import VectorStore
from llm_model import LLMModel
from rag_chain import RAGChain

# --- Cáº¤U HÃŒNH GIAO DIá»†N ---
st.set_page_config(page_title="ğŸ“˜ AI Chatbot tá»« PDF", layout="wide")
st.title("ğŸ“˜ AI Chatbot Há»— Trá»£ Há»c Táº­p tá»« PDF")
st.markdown("Trá»£ lÃ½ áº£o cÃ³ kháº£ nÄƒng Ä‘á»c file PDF vÃ  tráº£ lá»i cÃ¢u há»i dá»±a trÃªn ná»™i dung tÃ i liá»‡u báº¡n cung cáº¥p.")

# --- CACHING CÃC MODEL Tá»N KÃ‰M TÃ€I NGUYÃŠN ---
@st.cache_resource
def load_llm_model():
    st.info("â³ Äang táº£i mÃ´ hÃ¬nh ngÃ´n ngá»¯ (LLM)... Láº§n Ä‘áº§u cÃ³ thá»ƒ máº¥t vÃ i phÃºt.")
    model = LLMModel(model_name="vinai/PhoGPT-4B-Chat")
    return model

@st.cache_resource
def load_embedding_model():
    st.info("â³ Äang táº£i mÃ´ hÃ¬nh Embedding...")
    embedder = Embedder(model_name="sentence-transformers/all-MiniLM-L6-v2")
    return embedder

llm = load_llm_model()
embedder = load_embedding_model()

# --- QUáº¢N LÃ TRáº NG THÃI SESSION ---
if "rag_chain" not in st.session_state:
    st.session_state.rag_chain = None
if "messages" not in st.session_state:
    st.session_state.messages = []

# --- GIAO DIá»†N UPLOAD VÃ€ Xá»¬ LÃ ---
with st.sidebar:
    st.header("TÃ i liá»‡u cá»§a báº¡n")
    uploaded_file = st.file_uploader("ğŸ“ Táº£i lÃªn tÃ i liá»‡u PDF", type="pdf", label_visibility="collapsed")

    if uploaded_file:
        if st.button("Xá»­ lÃ½ tÃ i liá»‡u"):
            with st.spinner("ğŸ“– Äang Ä‘á»c vÃ  xá»­ lÃ½ tÃ i liá»‡u..."):
                try:
                    # Chuyá»ƒn Ä‘á»•i file upload thÃ nh Ä‘á»‘i tÆ°á»£ng Class
                    pdf_reader_instance = PDFReader()
                    text_content = pdf_reader_instance.read(uploaded_file)
                    
                    if not text_content or not text_content.strip():
                        st.error("KhÃ´ng thá»ƒ trÃ­ch xuáº¥t ná»™i dung tá»« file PDF. Vui lÃ²ng thá»­ file khÃ¡c.")
                    else:
                        text_splitter_instance = TextSplitter(chunk_size=1000, chunk_overlap=100)
                        chunks = text_splitter_instance.split(text_content)
                        
                        vector_store = VectorStore()
                        embeddings = embedder.embed_documents(chunks)
                        vector_store.add_embeddings(embeddings, chunks)

                        st.session_state.rag_chain = RAGChain(embedder, vector_store, llm)
                        st.session_state.messages = []
                        st.success("âœ… TÃ i liá»‡u Ä‘Ã£ Ä‘Æ°á»£c xá»­ lÃ½ xong! Báº¡n cÃ³ thá»ƒ báº¯t Ä‘áº§u trÃ² chuyá»‡n.")

                except Exception as e:
                    st.error(f"ÄÃ£ xáº£y ra lá»—i: {e}")

# --- GIAO DIá»†N CHAT ---
st.header("ğŸ’¬ Báº¯t Ä‘áº§u trÃ² chuyá»‡n")

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("Äáº·t cÃ¢u há»i vá» tÃ i liá»‡u cá»§a báº¡n..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    if st.session_state.rag_chain is None:
        with st.chat_message("assistant"):
            st.warning("Vui lÃ²ng táº£i lÃªn vÃ  nháº¥n nÃºt 'Xá»­ lÃ½ tÃ i liá»‡u' trÆ°á»›c khi Ä‘áº·t cÃ¢u há»i.")
    else:
        with st.chat_message("assistant"):
            with st.spinner("ğŸ¤– Äang suy nghÄ©..."):
                response = st.session_state.rag_chain.query(prompt)
                st.markdown(response)
        st.session_state.messages.append({"role": "assistant", "content": response})
