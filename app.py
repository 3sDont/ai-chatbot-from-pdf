# app.py

import streamlit as st
import os
import sys

# ThÃªm thÆ° má»¥c gá»‘c vÃ o sys.path Ä‘á»ƒ import tá»« src
project_root = os.path.dirname(os.path.abspath(__file__))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from src.components.data_loader import DataLoader
from src.components.chunker import Chunker
from src.components.embedder import Embedder
from src.components.vector_store import VectorStore
from src.pipelines.llm_models import GroqLLM
from src.pipelines.rag_pipeline import RAGPipeline

# --- Cáº¥u hÃ¬nh trang ---
st.set_page_config(page_title="ğŸ“š AI Chatbot", layout="centered", initial_sidebar_state="auto")
st.title("ğŸ“š AI Chatbot")
st.markdown("Trá»£ lÃ½ AI giÃºp báº¡n há»i Ä‘Ã¡p vÃ  khai thÃ¡c thÃ´ng tin tá»« tÃ i liá»‡u cá»§a mÃ¬nh.")

# --- Khá»Ÿi táº¡o cÃ¡c Ä‘á»‘i tÆ°á»£ng (dÃ¹ng cache Ä‘á»ƒ tá»‘i Æ°u) ---
@st.cache_resource
def initialize_models():
    """Táº£i model embedding vÃ  khá»Ÿi táº¡o káº¿t ná»‘i LLM."""
    try:
        embedder = Embedder()
        llm = GroqLLM()
        return embedder, llm
    except ValueError as e:
        st.error(f"Lá»—i khá»Ÿi táº¡o: {e}. Vui lÃ²ng kiá»ƒm tra API key trong Streamlit Secrets.")
        return None, None

def initialize_rag_pipeline(embedder, llm):
    """Khá»Ÿi táº¡o pipeline RAG cho má»—i session."""
    return RAGPipeline(Chunker(), embedder, VectorStore(), llm)

# Táº£i model vÃ  kiá»ƒm tra lá»—i
embedder_model, llm_model = initialize_models()
if not (embedder_model and llm_model):
    st.stop()

# Khá»Ÿi táº¡o pipeline vÃ  cÃ¡c biáº¿n session state
if 'rag_pipeline' not in st.session_state:
    st.session_state.rag_pipeline = initialize_rag_pipeline(embedder_model, llm_model)
if 'messages' not in st.session_state:
    st.session_state.messages = []
if 'document_processed' not in st.session_state:
    st.session_state.document_processed = False

# --- Giao diá»‡n Sidebar Ä‘á»ƒ Upload vÃ  Xá»­ lÃ½ ---
with st.sidebar:
    st.header("âš™ï¸ Báº£ng Äiá»u Khiá»ƒn")
    
    uploaded_file = st.file_uploader("Táº£i lÃªn tÃ i liá»‡u (PDF/MD)", type=["pdf", "md"])

    if uploaded_file:
        if st.session_state.get("last_file_name") != uploaded_file.name:
            st.session_state.last_file_name = uploaded_file.name
            st.session_state.document_processed = False
            st.session_state.messages = [{"role": "assistant", "content": f"ChÃ o báº¡n, tÃ´i Ä‘Ã£ sáºµn sÃ ng Ä‘á»ƒ tráº£ lá»i cÃ¡c cÃ¢u há»i vá» tÃ i liá»‡u '{uploaded_file.name}'."}]

            with st.status("âš™ï¸ Äang xá»­ lÃ½ tÃ i liá»‡u...", expanded=True) as status:
                status.write("Äang Ä‘á»c file...")
                loader = DataLoader()
                content = loader.load_from_upload(uploaded_file)
                
                if content:
                    status.write("Äang phÃ¢n tÃ­ch vÃ  ghi nhá»› ná»™i dung...")
                    st.session_state.rag_pipeline = initialize_rag_pipeline(embedder_model, llm_model)
                    st.session_state.rag_pipeline.setup_with_text(content)
                    st.session_state.document_processed = True
                    status.update(label="âœ… Xá»­ lÃ½ hoÃ n táº¥t!", state="complete", expanded=False)
                else:
                    status.update(label="Lá»—i Ä‘á»c file", state="error")

# --- Giao diá»‡n Chat ChÃ­nh (Má»™t cá»™t) ---
if not st.session_state.messages:
    st.info("ChÃ o má»«ng báº¡n! Vui lÃ²ng táº£i lÃªn má»™t tÃ i liá»‡u á»Ÿ thanh bÃªn Ä‘á»ƒ báº¯t Ä‘áº§u.")

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("Äáº·t cÃ¢u há»i vá» tÃ i liá»‡u..."):
    if not st.session_state.document_processed:
        st.warning("Vui lÃ²ng táº£i lÃªn vÃ  chá» xá»­ lÃ½ tÃ i liá»‡u trÆ°á»›c.")
    else:
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            with st.spinner("ğŸ¤– AI Ä‘ang suy nghÄ©..."):
                response = st.session_state.rag_pipeline.query(prompt)
                st.markdown(response)
        st.session_state.messages.append({"role": "assistant", "content": response})
