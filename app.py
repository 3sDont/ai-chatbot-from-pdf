# app.py

import streamlit as st
import os
import sys

# ====================================================================
# GIáº¢I QUYáº¾T Váº¤N Äá»€ IMPORT KHI DEPLOY
# ThÃªm thÆ° má»¥c gá»‘c cá»§a dá»± Ã¡n vÃ o sys.path Ä‘á»ƒ Python tÃ¬m tháº¥y 'src'
project_root = os.path.dirname(os.path.abspath(__file__))
if project_root not in sys.path:
    sys.path.insert(0, project_root)
# ====================================================================

from src.pdf_reader import PDFReader
from src.text_splitter import TextSplitter
from src.embedder import Embedder
from src.vector_store import VectorStore
from src.llm_model import LLMModel
from src.rag_chain import RAGChain

st.set_page_config(page_title="ğŸ“˜ AI Chatbot tá»« PDF", layout="wide")
st.title("ğŸ“˜ AI Chatbot Há»— Trá»£ Há»c Táº­p tá»« PDF")
st.markdown("Trá»£ lÃ½ áº£o cÃ³ kháº£ nÄƒng Ä‘á»c vÃ  hiá»ƒu ná»™i dung tá»« tÃ i liá»‡u PDF báº¡n cung cáº¥p.")

@st.cache_resource
def load_models():
    """Táº£i vÃ  cache táº¥t cáº£ cÃ¡c model náº·ng má»™t láº§n duy nháº¥t."""
    st.info("â³ Äang táº£i cÃ¡c mÃ´ hÃ¬nh AI... Láº§n Ä‘áº§u cÃ³ thá»ƒ máº¥t vÃ i phÃºt.")
    embedder = Embedder()
    # DÃ¹ng model máº·c Ä‘á»‹nh trong LLMModel lÃ  google/flan-t5-base
    llm = LLMModel() 
    st.success("âœ… CÃ¡c mÃ´ hÃ¬nh Ä‘Ã£ sáºµn sÃ ng!")
    return embedder, llm

embedder, llm = load_models()

if "rag_chain" not in st.session_state:
    st.session_state.rag_chain = None
if "messages" not in st.session_state:
    st.session_state.messages = []

with st.sidebar:
    st.header("TÃ i liá»‡u cá»§a báº¡n")
    uploaded_file = st.file_uploader("ğŸ“ Táº£i lÃªn tÃ i liá»‡u PDF", type="pdf", label_visibility="collapsed")

    if uploaded_file:
        if st.button("Xá»­ lÃ½ tÃ i liá»‡u"):
            with st.spinner("ğŸ“– Äang Ä‘á»c, phÃ¢n tÃ­ch vÃ  ghi nhá»› tÃ i liá»‡u..."):
                try:
                    pdf_reader = PDFReader()
                    text_content = pdf_reader.read(uploaded_file)

                    if not text_content or not text_content.strip():
                        st.error("KhÃ´ng thá»ƒ trÃ­ch xuáº¥t ná»™i dung tá»« file PDF. Vui lÃ²ng thá»­ file khÃ¡c.")
                    else:
                        text_splitter = TextSplitter(chunk_size=1000, chunk_overlap=100)
                        chunks = text_splitter.split(text_content)
                        
                        vector_store = VectorStore()
                        embeddings = embedder.embed_documents(chunks)
                        vector_store.add_documents(chunks, embeddings)

                        st.session_state.rag_chain = RAGChain(llm, vector_store, embedder)
                        st.session_state.messages = []
                        st.success("âœ… ÄÃ£ xá»­ lÃ½ xong! Báº¡n cÃ³ thá»ƒ báº¯t Ä‘áº§u trÃ² chuyá»‡n.")
                except Exception as e:
                    st.error(f"ÄÃ£ xáº£y ra lá»—i: {e}")

st.header("ğŸ’¬ Báº¯t Ä‘áº§u trÃ² chuyá»‡n")

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("Äáº·t cÃ¢u há»i vá» tÃ i liá»‡u..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    if st.session_state.rag_chain is None:
        st.warning("Vui lÃ²ng táº£i lÃªn vÃ  xá»­ lÃ½ má»™t file PDF trÆ°á»›c khi Ä‘áº·t cÃ¢u há»i.")
    else:
        with st.chat_message("assistant"):
            with st.spinner("ğŸ¤– Äang suy nghÄ©..."):
                response = st.session_state.rag_chain.query(prompt)
                st.markdown(response)
        st.session_state.messages.append({"role": "assistant", "content": response})
