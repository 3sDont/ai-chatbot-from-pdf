# app.py

import streamlit as st
import os
import sys

# ThÃªm thÆ° má»¥c gá»‘c vÃ o sys.path
project_root = os.path.dirname(os.path.abspath(__file__))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# Import tá»« kiáº¿n trÃºc má»›i
from src.components.data_loader import DataLoader
from src.components.chunker import Chunker
from src.components.embedder import Embedder
from src.components.vector_store import VectorStore
from src.pipelines.llm_models import FlanT5
from src.pipelines.rag_pipeline import RAGPipeline

st.set_page_config(page_title="ğŸ“š AI Chatbot SiÃªu Cáº¥p", layout="wide")
st.title("ğŸ“š AI Chatbot SiÃªu Cáº¥p")

# --- KHá»I Táº O CÃC Äá»I TÆ¯á»¢NG (DÃ™NG CACHE) ---

@st.cache_resource
def initialize_models():
    """Táº£i cÃ¡c model AI náº·ng."""
    embedder = Embedder()
    llm = FlanT5()
    return embedder, llm

# Khá»Ÿi táº¡o pipeline RAG má»™t láº§n cho má»—i session
# DÃ¹ng session_state thay vÃ¬ cache_resource Ä‘á»ƒ cÃ³ thá»ƒ reset pipeline khi Ä‘á»•i file
def initialize_rag_pipeline(embedder, llm):
    chunker = Chunker()
    vector_store = VectorStore()
    return RAGPipeline(chunker, embedder, vector_store, llm)

embedder_model, llm_model = initialize_models()
if 'rag_pipeline' not in st.session_state:
    st.session_state.rag_pipeline = initialize_rag_pipeline(embedder_model, llm_model)

# --- GIAO DIá»†N UPLOAD VÃ€ Xá»¬ LÃ ---
with st.sidebar:
    st.header("TÃ i liá»‡u cá»§a báº¡n")
    st.info(
        "**Lá»±a chá»n 1 (Nhanh):** Táº£i trá»±c tiáº¿p file PDF Ä‘á»ƒ cÃ³ cÃ¢u tráº£ lá»i ngay.\n\n"
        "**Lá»±a chá»n 2 (Cháº¥t lÆ°á»£ng cao):** Sá»­ dá»¥ng script `preprocess_pdf.py` trÃªn mÃ¡y cá»§a báº¡n Ä‘á»ƒ chuyá»ƒn PDF thÃ nh Markdown, sau Ä‘Ã³ táº£i file Markdown lÃªn."
    )
    
    uploaded_file = st.file_uploader("Táº£i lÃªn file PDF hoáº·c Markdown", type=["pdf", "md"])

    if uploaded_file is not None:
        # Sá»­ dá»¥ng session_state Ä‘á»ƒ lÆ°u tÃªn file vÃ  trÃ¡nh xá»­ lÃ½ láº¡i khÃ´ng cáº§n thiáº¿t
        if st.session_state.get("last_uploaded_filename") != uploaded_file.name:
            st.session_state.last_uploaded_filename = uploaded_file.name
            st.session_state.document_processed = False

        if not st.session_state.get("document_processed", False):
            with st.spinner("Äang náº¡p vÃ  xá»­ lÃ½ tÃ i liá»‡u..."):
                loader = DataLoader()
                content = ""
                file_type = uploaded_file.type
                
                if "pdf" in file_type:
                    st.write("Äang xá»­ lÃ½ file PDF (cháº¿ Ä‘á»™ nhanh)...")
                    content = loader.load_from_upload(uploaded_file)
                elif "markdown" in file_type or "text" in file_type:
                    st.write("Äang xá»­ lÃ½ file Markdown (cháº¿ Ä‘á»™ cháº¥t lÆ°á»£ng cao)...")
                    # Streamlit Ä‘á»c file text/md dÆ°á»›i dáº¡ng string
                    content = uploaded_file.getvalue().decode("utf-8")
                
                if content:
                    # Khá»Ÿi táº¡o láº¡i pipeline Ä‘á»ƒ xÃ³a dá»¯ liá»‡u cÅ©
                    st.session_state.rag_pipeline = initialize_rag_pipeline(embedder_model, llm_model)
                    st.session_state.rag_pipeline.setup_with_text(content)
                    st.session_state.document_processed = True
                    st.session_state.messages = [] # XÃ³a lá»‹ch sá»­ chat cÅ©
                    st.success("TÃ i liá»‡u Ä‘Ã£ Ä‘Æ°á»£c náº¡p vÃ  sáºµn sÃ ng Ä‘á»ƒ há»i Ä‘Ã¡p.")
                else:
                    st.error("KhÃ´ng thá»ƒ Ä‘á»c ná»™i dung tá»« file Ä‘Æ°á»£c táº£i lÃªn.")

# --- GIAO DIá»†N CHAT ---
st.header("ğŸ’¬ TrÃ² chuyá»‡n vá»›i tÃ i liá»‡u")

if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("Äáº·t cÃ¢u há»i vá» tÃ i liá»‡u..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    if not st.session_state.get("document_processed", False):
        st.warning("Vui lÃ²ng táº£i lÃªn vÃ  chá» xá»­ lÃ½ tÃ i liá»‡u trÆ°á»›c khi Ä‘áº·t cÃ¢u há»i.")
    else:
        with st.chat_message("assistant"):
            with st.spinner("ğŸ¤– Äang suy nghÄ©..."):
                response = st.session_state.rag_pipeline.query(prompt)
                st.markdown(response)
        st.session_state.messages.append({"role": "assistant", "content": response})
