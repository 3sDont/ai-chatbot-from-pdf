# app.py

import streamlit as st
import os
from src.pdf_reader import PDFReader
from src.text_splitter import TextSplitter
from src.embedder import Embedder
from src.vector_store import VectorStore
from src.llm_model import LLMModel
from src.rag_chain import RAGChain

# --- Cáº¤U HÃŒNH GIAO DIá»†N ---
st.set_page_config(page_title="ğŸ“˜ AI Chatbot tá»« PDF", layout="wide")
st.title("ğŸ“˜ AI Chatbot Há»— Trá»£ Há»c Táº­p tá»« PDF")
st.markdown("Trá»£ lÃ½ áº£o cÃ³ kháº£ nÄƒng Ä‘á»c file PDF vÃ  tráº£ lá»i cÃ¢u há»i dá»±a trÃªn ná»™i dung tÃ i liá»‡u báº¡n cung cáº¥p.")

# --- CACHING CÃC MODEL Tá»N KÃ‰M TÃ€I NGUYÃŠN ---
@st.cache_resource
def load_llm_model():
    st.write("â³ Äang táº£i mÃ´ hÃ¬nh ngÃ´n ngá»¯ (LLM)... Láº§n Ä‘áº§u cÃ³ thá»ƒ máº¥t vÃ i phÃºt.")
    # Báº¡n cÃ³ thá»ƒ Ä‘á»•i láº¡i model microsoft/phi-2 náº¿u muá»‘n, nhÆ°ng cáº§n trust_remote_code=True
    model = LLMModel(model_name="vinai/PhoGPT-4B-Chat")
    st.write("âœ… ÄÃ£ táº£i xong mÃ´ hÃ¬nh LLM.")
    return model

@st.cache_resource
def load_embedding_model():
    st.write("â³ Äang táº£i mÃ´ hÃ¬nh Embedding...")
    embedder = Embedder(model_name="sentence-transformers/all-MiniLM-L6-v2")
    st.write("âœ… ÄÃ£ táº£i xong mÃ´ hÃ¬nh Embedding.")
    return embedder

llm = load_llm_model()
embedder = load_embedding_model()

# --- QUáº¢N LÃ TRáº NG THÃI SESSION ---
if "rag_chain" not in st.session_state:
    st.session_state.rag_chain = None
if "messages" not in st.session_state:
    st.session_state.messages = []

# --- GIAO DIá»†N UPLOAD VÃ€ Xá»¬ LÃ ---
with st.sidebar:
    st.header("TÃ i liá»‡u cá»§a báº¡n")
    uploaded_file = st.file_uploader("ğŸ“ Táº£i lÃªn tÃ i liá»‡u PDF", type="pdf", label_visibility="collapsed")

    if uploaded_file:
        if st.button("Xá»­ lÃ½ tÃ i liá»‡u"):
            with st.spinner("ğŸ“– Äang Ä‘á»c vÃ  xá»­ lÃ½ tÃ i liá»‡u..."):
                try:
                    # 1. Äá»c PDF
                    pdf_reader = PDFReader()
                    text_content = pdf_reader.read(uploaded_file)

                    # 2. Chia nhá» vÄƒn báº£n
                    text_splitter = TextSplitter(chunk_size=1000, chunk_overlap=100)
                    chunks = text_splitter.split(text_content)

                    if not chunks:
                         st.error("KhÃ´ng thá»ƒ trÃ­ch xuáº¥t ná»™i dung tá»« PDF. Vui lÃ²ng thá»­ file khÃ¡c.")
                    else:
                        # 3. Táº¡o embedding vÃ  lÆ°u vÃ o Vector Store
                        vector_store = VectorStore()
                        embeddings = embedder.embed_documents(chunks)
                        vector_store.add_embeddings(embeddings, chunks)

                        # 4. Táº¡o RAG chain vÃ  lÆ°u vÃ o session state
                        st.session_state.rag_chain = RAGChain(embedder, vector_store, llm)

                        # XÃ³a lá»‹ch sá»­ chat cÅ©
                        st.session_state.messages = []
                        st.success("âœ… TÃ i liá»‡u Ä‘Ã£ Ä‘Æ°á»£c xá»­ lÃ½ xong! Báº¡n cÃ³ thá»ƒ báº¯t Ä‘áº§u trÃ² chuyá»‡n.")
                except Exception as e:
                    st.error(f"ÄÃ£ xáº£y ra lá»—i: {e}")

# --- GIAO DIá»†N CHAT ---
st.header("ğŸ’¬ Báº¯t Ä‘áº§u trÃ² chuyá»‡n")

# Hiá»ƒn thá»‹ cÃ¡c tin nháº¯n Ä‘Ã£ cÃ³
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Nháº­n input tá»« ngÆ°á»i dÃ¹ng
if prompt := st.chat_input("Äáº·t cÃ¢u há»i vá» tÃ i liá»‡u cá»§a báº¡n..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    if st.session_state.rag_chain is None:
        with st.chat_message("assistant"):
            st.warning("Vui lÃ²ng táº£i lÃªn vÃ  xá»­ lÃ½ má»™t file PDF trÆ°á»›c khi Ä‘áº·t cÃ¢u há»i.")
    else:
        with st.chat_message("assistant"):
            with st.spinner("ğŸ¤– Äang suy nghÄ©..."):
                response = st.session_state.rag_chain.query(prompt)
                st.markdown(response)
        st.session_state.messages.append({"role": "assistant", "content": response})
