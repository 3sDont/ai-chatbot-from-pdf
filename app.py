# app.py

import streamlit as st
import os
import sys

# ThÃªm thÆ° má»¥c gá»‘c vÃ o sys.path
project_root = os.path.dirname(os.path.abspath(__file__))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# Import tá»« kiáº¿n trÃºc má»›i
from src.components.data_loader import DataLoader
from src.components.chunker import Chunker
from src.components.embedder import Embedder
from src.components.vector_store import VectorStore
from src.pipelines.llm_models import FlanT5
from src.pipelines.rag_pipeline import RAGPipeline

st.set_page_config(page_title="ğŸ“š AI Chatbot SiÃªu Cáº¥p", layout="wide")
st.title("ğŸ“š AI Chatbot SiÃªu Cáº¥p")

# --- KHá»I Táº O CÃC Äá»I TÆ¯á»¢NG (DÃ™NG CACHE) ---

@st.cache_resource
def initialize_models():
    """Táº£i cÃ¡c model AI náº·ng."""
    st.info("Äang táº£i cÃ¡c mÃ´ hÃ¬nh AI...")
    embedder = Embedder()
    llm = FlanT5()
    st.success("MÃ´ hÃ¬nh AI Ä‘Ã£ sáºµn sÃ ng.")
    return embedder, llm

@st.cache_resource
def initialize_pipeline(_embedder, _llm):
    """Khá»Ÿi táº¡o pipeline RAG."""
    # CÃ¡c thÃ nh pháº§n khÃ´ng cáº§n model AI cÃ³ thá»ƒ khá»Ÿi táº¡o á»Ÿ Ä‘Ã¢y
    chunker = Chunker()
    vector_store = VectorStore()
    rag_pipeline = RAGPipeline(chunker, _embedder, vector_store, _llm)
    return rag_pipeline

# Táº£i model vÃ  khá»Ÿi táº¡o pipeline
embedder_model, llm_model = initialize_models()
rag_pipeline = initialize_pipeline(embedder_model, llm_model)

# --- Xá»¬ LÃ TÃ€I LIá»†U ---

# Sá»­ dá»¥ng session_state Ä‘á»ƒ Ä‘Ã¡nh dáº¥u tÃ i liá»‡u Ä‘Ã£ Ä‘Æ°á»£c xá»­ lÃ½ hay chÆ°a
if "document_processed" not in st.session_state:
    st.session_state.document_processed = False

with st.sidebar:
    st.header("TÃ i liá»‡u")
    # Thay vÃ¬ upload, chÃºng ta chá»n file Ä‘Ã£ xá»­ lÃ½
    # Trong thá»±c táº¿, báº¡n cÃ³ thá»ƒ táº¡o má»™t dropdown Ä‘á»ƒ chá»n tá»« cÃ¡c file trong `documents/markdowns`
    processed_doc_path = "documents/markdowns/your_document.md" # <-- THAY TÃŠN FILE

    if st.button("Náº¡p vÃ  Xá»­ lÃ½ TÃ i liá»‡u"):
        if os.path.exists(processed_doc_path):
            with st.spinner("Äang náº¡p vÃ  xá»­ lÃ½ tÃ i liá»‡u..."):
                loader = DataLoader()
                content = loader.load(processed_doc_path)
                rag_pipeline.setup_with_text(content)
                st.session_state.document_processed = True
            st.success("TÃ i liá»‡u Ä‘Ã£ Ä‘Æ°á»£c náº¡p vÃ  sáºµn sÃ ng Ä‘á»ƒ há»i Ä‘Ã¡p.")
        else:
            st.error(f"File khÃ´ng tá»“n táº¡i: {processed_doc_path}. Vui lÃ²ng cháº¡y script `preprocess_pdf.py` trÆ°á»›c.")

# --- GIAO DIá»†N CHAT ---

st.header("ğŸ’¬ TrÃ² chuyá»‡n vá»›i tÃ i liá»‡u")

if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("Äáº·t cÃ¢u há»i vá» tÃ i liá»‡u..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    if not st.session_state.document_processed:
        st.warning("Vui lÃ²ng nháº¥n nÃºt 'Náº¡p vÃ  Xá»­ lÃ½ TÃ i liá»‡u' á»Ÿ thanh bÃªn trÆ°á»›c.")
    else:
        with st.chat_message("assistant"):
            with st.spinner("ğŸ¤– Äang suy nghÄ©..."):
                response = rag_pipeline.query(prompt)
                st.markdown(response)
        st.session_state.messages.append({"role": "assistant", "content": response})
