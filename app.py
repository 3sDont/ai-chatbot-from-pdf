# app.py

import streamlit as st
import os
import sys

project_root = os.path.dirname(os.path.abspath(__file__))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from src.pdf_reader import PDFReader
from src.text_splitter import TextSplitter
from src.embedder import Embedder
from src.vector_store import VectorStore
from src.llm_model import LLMModel
from src.rag_chain import RAGChain

# --- Cáº¤U HÃŒNH GIAO DIá»†N ---
st.set_page_config(page_title="ğŸ“˜ AI Chatbot tá»« PDF", layout="wide")
st.title("ğŸ“˜ AI Chatbot Há»— Trá»£ Há»c Táº­p tá»« PDF")
st.markdown("Trá»£ lÃ½ áº£o cÃ³ kháº£ nÄƒng Ä‘á»c file PDF vÃ  tráº£ lá»i cÃ¢u há»i dá»±a trÃªn ná»™i dung tÃ i liá»‡u báº¡n cung cáº¥p.")

# --- CACHING CÃC MODEL Tá»N KÃ‰M TÃ€I NGUYÃŠN ---
@st.cache_resource
def load_llm_model():
    """Táº£i vÃ  cache mÃ´ hÃ¬nh LLM. Chá»‰ cháº¡y má»™t láº§n duy nháº¥t."""
    st.write("â³ Äang táº£i mÃ´ hÃ¬nh ngÃ´n ngá»¯ (LLM)... Láº§n Ä‘áº§u cÃ³ thá»ƒ máº¥t vÃ i phÃºt.")
    # Sá»­ dá»¥ng model vinai/PhoGPT-4B-Chat vÃ¬ nÃ³ tá»‘t cho tiáº¿ng Viá»‡t
    # trust_remote_code=True lÃ  cáº§n thiáº¿t cho má»™t sá»‘ model
    model = LLMModel(model_name="vinai/PhoGPT-4B-Chat")
    st.write("âœ… ÄÃ£ táº£i xong mÃ´ hÃ¬nh LLM.")
    return model

@st.cache_resource
def load_embedding_model():
    """Táº£i vÃ  cache mÃ´ hÃ¬nh Embedding. Chá»‰ cháº¡y má»™t láº§n duy nháº¥t."""
    st.write("â³ Äang táº£i mÃ´ hÃ¬nh Embedding...")
    embedder = Embedder(model_name="sentence-transformers/all-MiniLM-L6-v2")
    st.write("âœ… ÄÃ£ táº£i xong mÃ´ hÃ¬nh Embedding.")
    return embedder

# Táº£i cÃ¡c model
llm = load_llm_model()
embedder = load_embedding_model()

# --- QUáº¢N LÃ TRáº NG THÃI SESSION ---
# DÃ¹ng st.session_state Ä‘á»ƒ lÆ°u trá»¯ dá»¯ liá»‡u giá»¯a cÃ¡c láº§n cháº¡y láº¡i script
if "rag_chain" not in st.session_state:
    st.session_state.rag_chain = None
if "messages" not in st.session_state:
    st.session_state.messages = []

# --- GIAO DIá»†N UPLOAD VÃ€ Xá»¬ LÃ ---
with st.sidebar:
    st.header("TÃ i liá»‡u cá»§a báº¡n")
    uploaded_file = st.file_uploader("ğŸ“ Táº£i lÃªn tÃ i liá»‡u PDF", type="pdf", label_visibility="collapsed")

    if uploaded_file:
        if st.button("Xá»­ lÃ½ tÃ i liá»‡u"):
            with st.spinner("ğŸ“– Äang Ä‘á»c vÃ  xá»­ lÃ½ tÃ i liá»‡u... Viá»‡c nÃ y cÃ³ thá»ƒ máº¥t má»™t lÃºc."):
                try:
                    # 1. Äá»c PDF
                    pdf_reader = PDFReader()
                    text_content = pdf_reader.read(uploaded_file)

                    # 2. Chia nhá» vÄƒn báº£n
                    # (LÆ°u Ã½: Báº¡n Ä‘ang dÃ¹ng class TextSplitter tá»« code cá»§a báº¡n, khÃ´ng pháº£i langchain)
                    text_splitter = TextSplitter(chunk_size=1000, chunk_overlap=100)
                    chunks = text_splitter.split(text_content)

                    if not chunks or not any(chunk.strip() for chunk in chunks):
                         st.error("KhÃ´ng thá»ƒ trÃ­ch xuáº¥t ná»™i dung vÄƒn báº£n tá»« file PDF. Vui lÃ²ng thá»­ file khÃ¡c.")
                    else:
                        # 3. Táº¡o embedding vÃ  lÆ°u vÃ o Vector Store
                        vector_store = VectorStore()
                        st.write("Táº¡o embeddings cho cÃ¡c Ä‘oáº¡n vÄƒn báº£n...")
                        embeddings = embedder.embed_documents(chunks)
                        vector_store.add_embeddings(embeddings, chunks)

                        # 4. Táº¡o RAG chain vÃ  lÆ°u vÃ o session state Ä‘á»ƒ dÃ¹ng láº¡i
                        st.session_state.rag_chain = RAGChain(embedder, vector_store, llm)
                        
                        # XÃ³a lá»‹ch sá»­ chat cÅ© khi cÃ³ tÃ i liá»‡u má»›i
                        st.session_state.messages = []
                        st.success("âœ… TÃ i liá»‡u Ä‘Ã£ Ä‘Æ°á»£c xá»­ lÃ½ xong! Báº¡n cÃ³ thá»ƒ báº¯t Ä‘áº§u trÃ² chuyá»‡n.")
                except Exception as e:
                    st.error(f"ÄÃ£ xáº£y ra lá»—i trong quÃ¡ trÃ¬nh xá»­ lÃ½: {e}")

# --- GIAO DIá»†N CHAT ---
st.header("ğŸ’¬ Báº¯t Ä‘áº§u trÃ² chuyá»‡n")

# Hiá»ƒn thá»‹ cÃ¡c tin nháº¯n Ä‘Ã£ cÃ³ trong lá»‹ch sá»­
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Nháº­n input má»›i tá»« ngÆ°á»i dÃ¹ng
if prompt := st.chat_input("Äáº·t cÃ¢u há»i vá» tÃ i liá»‡u cá»§a báº¡n..."):
    # ThÃªm tin nháº¯n cá»§a ngÆ°á»i dÃ¹ng vÃ o lá»‹ch sá»­ vÃ  hiá»ƒn thá»‹
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # Kiá»ƒm tra xem tÃ i liá»‡u Ä‘Ã£ Ä‘Æ°á»£c xá»­ lÃ½ chÆ°a
    if st.session_state.rag_chain is None:
        with st.chat_message("assistant"):
            st.warning("Vui lÃ²ng táº£i lÃªn vÃ  nháº¥n nÃºt 'Xá»­ lÃ½ tÃ i liá»‡u' trÆ°á»›c khi Ä‘áº·t cÃ¢u há»i.")
    else:
        # Táº¡o vÃ  hiá»ƒn thá»‹ cÃ¢u tráº£ lá»i cá»§a bot
        with st.chat_message("assistant"):
            with st.spinner("ğŸ¤– Äang suy nghÄ©..."):
                response = st.session_state.rag_chain.query(prompt)
                st.markdown(response)
        
        # ThÃªm cÃ¢u tráº£ lá»i cá»§a bot vÃ o lá»‹ch sá»­
        st.session_state.messages.append({"role": "assistant", "content": response})
