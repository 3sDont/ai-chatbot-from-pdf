# app.py

import streamlit as st
import os
import sys

# ThÃªm thÆ° má»¥c gá»‘c vÃ o sys.path Ä‘á»ƒ import tá»« src
project_root = os.path.dirname(os.path.abspath(__file__))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from src.components.data_loader import DataLoader
from src.components.chunker import Chunker
from src.components.embedder import Embedder
from src.components.vector_store import VectorStore
from src.pipelines.llm_models import GroqLLM
from src.pipelines.rag_pipeline import RAGPipeline

# --- Cáº¥u hÃ¬nh trang ---
st.set_page_config(page_title="ğŸ“š AI Chatbot Pro", layout="wide", initial_sidebar_state="expanded")
st.title("ğŸ“š Há»i Ä‘i BaDong tráº£ lá»i cho")

# --- Khá»Ÿi táº¡o cÃ¡c Ä‘á»‘i tÆ°á»£ng (dÃ¹ng cache Ä‘á»ƒ tá»‘i Æ°u) ---
@st.cache_resource
def initialize_models():
    """Táº£i model embedding vÃ  khá»Ÿi táº¡o káº¿t ná»‘i LLM."""
    try:
        embedder = Embedder()
        llm = GroqLLM()
        return embedder, llm
    except ValueError as e:
        st.error(f"Lá»—i khá»Ÿi táº¡o: {e}. Vui lÃ²ng kiá»ƒm tra API key trong Streamlit Secrets.")
        return None, None

def initialize_rag_pipeline(embedder, llm):
    """Khá»Ÿi táº¡o pipeline RAG cho má»—i session."""
    return RAGPipeline(Chunker(), embedder, VectorStore(), llm)

# Táº£i model vÃ  kiá»ƒm tra lá»—i
embedder_model, llm_model = initialize_models()
if not (embedder_model and llm_model):
    st.stop()

# Khá»Ÿi táº¡o pipeline trong session state
if 'rag_pipeline' not in st.session_state:
    st.session_state.rag_pipeline = initialize_rag_pipeline(embedder_model, llm_model)
if 'messages' not in st.session_state:
    st.session_state.messages = []
if 'document_processed' not in st.session_state:
    st.session_state.document_processed = False

# --- Giao diá»‡n Sidebar Ä‘á»ƒ Upload vÃ  Xá»­ lÃ½ ---
with st.sidebar:
    st.header("âš™ï¸ Báº£ng Äiá»u Khiá»ƒn")
    st.info(
        "**BÆ°á»›c 1:** Táº£i lÃªn file PDF hoáº·c Markdown.\n\n"
        "**BÆ°á»›c 2:** Chá» xá»­ lÃ½ xong vÃ  báº¯t Ä‘áº§u há»i Ä‘Ã¡p!"
    )
    
    uploaded_file = st.file_uploader("Táº£i lÃªn tÃ i liá»‡u", type=["pdf", "md"])

    if uploaded_file:
        # Xá»­ lÃ½ chá»‰ khi cÃ³ file má»›i Ä‘Æ°á»£c táº£i lÃªn
        if st.session_state.get("last_file_name") != uploaded_file.name:
            st.session_state.last_file_name = uploaded_file.name
            st.session_state.document_processed = False
            st.session_state.messages = [] # Reset chat khi cÃ³ file má»›i

            with st.status("âš™ï¸ Äang xá»­ lÃ½ tÃ i liá»‡u...", expanded=True) as status:
                st.write("Äang Ä‘á»c file...")
                loader = DataLoader()
                content = loader.load_from_upload(uploaded_file)
                
                if content:
                    st.write("Äang phÃ¢n tÃ­ch vÃ  ghi nhá»› ná»™i dung...")
                    # Reset pipeline Ä‘á»ƒ náº¡p dá»¯ liá»‡u má»›i
                    st.session_state.rag_pipeline = initialize_rag_pipeline(embedder_model, llm_model)
                    st.session_state.rag_pipeline.setup_with_text(content)
                    st.session_state.document_processed = True
                    status.update(label="âœ… Xá»­ lÃ½ hoÃ n táº¥t!", state="complete", expanded=False)
                else:
                    status.update(label="Lá»—i Ä‘á»c file", state="error")
    

# --- Giao diá»‡n Chat ChÃ­nh ---
if not st.session_state.document_processed:
    st.info("ChÃ o má»«ng báº¡n! Vui lÃ²ng táº£i lÃªn má»™t tÃ i liá»‡u á»Ÿ thanh bÃªn Ä‘á»ƒ báº¯t Ä‘áº§u.")
else:
    st.success(f"ÄÃ£ sáºµn sÃ ng! Há»i Ä‘Ã¡p vá» tÃ i liá»‡u: **{st.session_state.last_file_name}**")

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("CÃ¢u há»i cá»§a báº¡n vá» tÃ i liá»‡u..."):
    if not st.session_state.document_processed:
        st.warning("Vui lÃ²ng táº£i lÃªn vÃ  chá» xá»­ lÃ½ tÃ i liá»‡u trÆ°á»›c.")
    else:
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            with st.spinner("ğŸ¤– AI Ä‘ang suy nghÄ©..."):
                response = st.session_state.rag_pipeline.query(prompt)
                st.markdown(response)
        st.session_state.messages.append({"role": "assistant", "content": response})
